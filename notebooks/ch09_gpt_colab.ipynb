{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Chapter 9 \u2014 Building the Model Step by Step (Colab)\n\n", "Self-contained notebook that assembles a compact GPT using the classes developed in the book. One creation per cell; show each created object right away."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Colab-friendly: ensure PyTorch is available\n", "import sys, subprocess\n", "try:\n", "    import torch  # noqa: F401\n", "    print('PyTorch found')\n", "except Exception:\n", "    print('Installing PyTorch...')\n", "    # Heuristic: if NVIDIA driver present, prefer a CUDA build\n", "    has_cuda = False\n", "    try:\n", "        r = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n", "        has_cuda = (r.returncode == 0)\n", "    except Exception:\n", "        has_cuda = False\n", "    if has_cuda:\n", "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', 'https://download.pytorch.org/whl/cu121', 'torch', 'torchvision', 'torchaudio'])\n", "    else:\n", "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', 'https://download.pytorch.org/whl/cpu', 'torch', 'torchvision', 'torchaudio'])\n", "    import torch  # noqa: F401\n", "print('torch', torch.__version__)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Imports, style, and a tiny seed\n", "import math, platform\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "plt.style.use('seaborn-v0_8')\n", "%config InlineBackend.figure_format = 'svg'\n", "torch.manual_seed(0)\n", "device = (\n    'cuda' if torch.cuda.is_available() else\n    'mps' if getattr(torch.backends, 'mps', None)\n            and torch.backends.mps.is_available()\n    else 'cpu'\n)\n", "device\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Sinusoidal positions (Chapter 8)\n", "def sinusoidal_positions(T: int, d_model: int, device: torch.device | None = None) -> torch.Tensor:\n", "    pos = torch.arange(T, device=device).float()[:, None]\n", "    i = torch.arange(d_model, device=device).float()[None, :]\n", "    angle = pos / (10000 ** (2 * (i//2) / d_model))\n", "    enc = torch.zeros(T, d_model, device=device)\n", "    enc[:, 0::2] = torch.sin(angle[:, 0::2])\n", "    enc[:, 1::2] = torch.cos(angle[:, 1::2])\n", "    return enc\n", "sinusoidal_positions(4, 8).shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Multi-head self-attention (Chapter 8)\n", "class MultiHeadAttention(nn.Module):\n", "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n", "        super().__init__()\n", "        assert d_model % num_heads == 0\n", "        self.h = num_heads\n", "        self.d = d_model // num_heads\n", "        self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n", "        self.out = nn.Linear(d_model, d_model, bias=False)\n", "        self.drop = nn.Dropout(dropout)\n", "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None):\n", "        B, T, Dm = x.shape\n", "        qkv = self.qkv(x)\n", "        q, k, v = qkv.chunk(3, dim=-1)\n", "        def split(t):\n", "            return t.view(B, T, self.h, self.d).transpose(1, 2)\n", "        q, k, v = map(split, (q, k, v))\n", "        sdpa_mask = None\n", "        if mask is not None:\n", "            if mask.dim() == 2:\n", "                base = (mask == 0).bool()[None, None, :, :]\n", "                sdpa_mask = base.expand(B, self.h, T, T)\n", "            elif mask.dim() == 3:\n", "                base = (mask == 0).bool().unsqueeze(1)\n", "                sdpa_mask = base.expand(B, self.h, T, T)\n", "            elif mask.dim() == 4:\n", "                if mask.size(1) == 1:\n", "                    sdpa_mask = (mask == 0).bool().expand(B, self.h, T, T)\n", "                else:\n", "                    sdpa_mask = (mask == 0).bool()\n", "        attn = F.scaled_dot_product_attention(q, k, v, attn_mask=sdpa_mask)\n", "        attn = self.drop(attn)\n", "        y = attn.transpose(1, 2).contiguous().view(B, T, Dm)\n", "        return self.out(y)\n", "MultiHeadAttention(16, 4)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Feed-forward (Chapter 8)\n", "class FeedForward(nn.Module):\n", "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0):\n", "        super().__init__()\n", "        self.net = nn.Sequential(\n", "            nn.Linear(d_model, d_ff),\n", "            nn.GELU(),\n", "            nn.Dropout(dropout),\n", "            nn.Linear(d_ff, d_model),\n", "            nn.Dropout(dropout),\n", "        )\n", "    def forward(self, x):\n", "        return self.net(x)\n", "FeedForward(16, 64, 0.1)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Residual wrapper (pre-norm)\n", "class Residual(nn.Module):\n", "    def __init__(self, d_model: int):\n", "        super().__init__()\n", "        self.norm = nn.LayerNorm(d_model)\n", "    def forward(self, x, sublayer: nn.Module, *args, **kwargs):\n", "        return x + sublayer(self.norm(x), *args, **kwargs)\n", "Residual(16)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Transformer block (Chapter 8)\n", "class TransformerBlock(nn.Module):\n", "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.0):\n", "        super().__init__()\n", "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n", "        self.ffn = FeedForward(d_model, d_ff, dropout)\n", "        self.res1 = Residual(d_model)\n", "        self.res2 = Residual(d_model)\n", "    def forward(self, x, mask=None):\n", "        x = self.res1(x, self.mha, mask)\n", "        x = self.res2(x, self.ffn)\n", "        return x\n", "TransformerBlock(16, 4, 64, 0.1)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# GPTConfig dataclass\n", "from dataclasses import dataclass\n", "@dataclass\n", "class GPTConfig:\n", "    vocab_size: int\n", "    block_size: int\n", "    d_model: int = 128\n", "    n_head: int = 4\n", "    n_layer: int = 2\n", "    d_ff: int = 512\n", "    dropout: float = 0.1\n", "    pos_type: str = 'learned'\n", "    tie_weights: bool = True\n", "cfg = GPTConfig(vocab_size=256, block_size=32)\n", "cfg\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# GPT model (compact)\n", "class GPT(nn.Module):\n", "    def __init__(self, cfg: GPTConfig):\n", "        super().__init__()\n", "        self.cfg = cfg\n", "        V, Tm, D = cfg.vocab_size, cfg.block_size, cfg.d_model\n", "        self.tok_emb = nn.Embedding(V, D)\n", "        self.pos_emb = nn.Embedding(Tm, D) if cfg.pos_type == 'learned' else None\n", "        self.drop = nn.Dropout(cfg.dropout)\n", "        self.blocks = nn.ModuleList([TransformerBlock(D, cfg.n_head, cfg.d_ff, cfg.dropout) for _ in range(cfg.n_layer)])\n", "        self.norm_f = nn.LayerNorm(D)\n", "        self.lm_head = nn.Linear(D, V, bias=False)\n", "        if cfg.tie_weights:\n", "            self.lm_head.weight = self.tok_emb.weight\n", "        self.apply(self._init_weights)\n", "    def _init_weights(self, m):\n", "        if isinstance(m, (nn.Linear, nn.Embedding)):\n", "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n", "        if isinstance(m, nn.Linear) and m.bias is not None:\n", "            nn.init.zeros_(m.bias)\n", "    def _build_mask(self, input_ids, attention_mask=None, pad_id=None):\n", "        B, T = input_ids.size()\n", "        device = input_ids.device\n", "        causal = torch.tril(torch.ones(T, T, device=device))\n", "        if attention_mask is not None:\n", "            pad_bt = attention_mask.float()\n", "        elif pad_id is not None:\n", "            pad_bt = (input_ids != pad_id).float()\n", "        else:\n", "            return causal.unsqueeze(0).expand(B, -1, -1)\n", "        return pad_bt[:, None, :] * causal\n", "    def forward(self, input_ids, targets=None, attention_mask=None, pad_id=None):\n", "        B, T = input_ids.size()\n", "        assert T <= self.cfg.block_size\n", "        x = self.tok_emb(input_ids)\n", "        if self.cfg.pos_type == 'learned':\n", "            positions = torch.arange(T, device=input_ids.device)[None, :]\n", "            x = x + self.pos_emb(positions)\n", "        else:\n", "            pe = sinusoidal_positions(T, self.cfg.d_model, device=input_ids.device)\n", "            x = x + pe[None, :, :]\n", "        x = self.drop(x)\n", "        mask = self._build_mask(input_ids, attention_mask, pad_id)\n", "        for block in self.blocks:\n", "            x = block(x, mask)\n", "        x = self.norm_f(x)\n", "        logits = self.lm_head(x)\n", "        loss = None\n", "        if targets is not None:\n", "            logits_f = logits.reshape(B*T, -1)\n", "            targets_f = targets.reshape(B*T)\n", "            ignore = pad_id if pad_id is not None else -100\n", "            loss = F.cross_entropy(logits_f, targets_f, ignore_index=ignore)\n", "        return logits, loss\n", "model = GPT(cfg).to(device); model\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create a tiny batch of random token ids and show shape\n", "B, T = 2, 16\n", "x = torch.randint(0, cfg.vocab_size, (B, T), device=device)\n", "x.shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Forward without targets: logits only\n", "with torch.no_grad():\n", "    logits, _ = model(x)\n", "logits.shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Forward with targets: compute next-token loss\n", "y = torch.randint(0, cfg.vocab_size, (B, T), device=device)\n", "_, loss = model(x, targets=y)\n", "float(loss.detach().cpu().item())\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tiny optimization sanity check: loss should decrease a bit\n", "opt = torch.optim.AdamW(model.parameters(), lr=3e-3)\n", "hist = []\n", "for step in range(5):\n", "    opt.zero_grad()\n", "    _, loss = model(x, targets=y)\n", "    loss.backward()\n", "    opt.step()\n", "    hist.append(float(loss.detach().cpu().item()))\n", "hist\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}