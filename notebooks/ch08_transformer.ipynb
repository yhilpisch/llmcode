{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Chapter 8 \u2014 The Transformer Architecture\n\n", "Multi\u2011head attention, residuals with LayerNorm, a feed\u2011forward, and sinusoidal positions. Small and testable."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ensure torch (Colab friendly)\n", "try:\n", "    import torch  # noqa\n", "    print('torch:', torch.__version__)\n", "except Exception:\n", "    import os\n", "    gpu = os.system('nvidia-smi > /dev/null 2>&1') == 0\n", "    index = 'https://download.pytorch.org/whl/cu121' if gpu else 'https://download.pytorch.org/whl/cpu'\n", "    get_ipython().run_line_magic('pip', f'install -q torch --index-url {index}')\n", "    import torch\n", "    print('torch:', torch.__version__)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "plt.style.use('seaborn-v0_8')\n", "%config InlineBackend.figure_format = 'svg'\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Positional encoding\n", "def sinusoidal_positions(T: int, d_model: int, device=None):\n", "    import math, torch\n", "    pos = torch.arange(T, device=device).float()[:, None]\n", "    i = torch.arange(d_model, device=device).float()[None, :]\n", "    angle = pos / (10000 ** (2 * (i // 2) / d_model))\n", "    enc = torch.zeros(T, d_model, device=device)\n", "    enc[:, 0::2] = torch.sin(angle[:, 0::2])\n", "    enc[:, 1::2] = torch.cos(angle[:, 1::2])\n", "    return enc\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Multi-head attention\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "class MultiHeadAttention(nn.Module):\n", "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n", "        super().__init__(); assert d_model % num_heads == 0\n", "        self.h = num_heads; self.d = d_model // num_heads\n", "        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n", "        self.out = nn.Linear(d_model, d_model, bias=False)\n", "        self.drop = nn.Dropout(dropout)\n", "    def forward(self, x, mask=None):\n", "        B, T, Dm = x.shape\n", "        qkv = self.qkv(x); q, k, v = qkv.chunk(3, dim=-1)\n", "        def split(t): return t.view(B, T, self.h, self.d).transpose(1, 2)\n", "        q, k, v = map(split, (q, k, v))\n", "        attn = F.scaled_dot_product_attention(q, k, v, attn_mask=mask)\n", "        attn = self.drop(attn)\n", "        y = attn.transpose(1,2).contiguous().view(B, T, Dm)\n", "        return self.out(y)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Check head shapes in isolation\n", "B, T, D, H = 2, 5, 12, 3\n", "x_chk = torch.randn(B, T, D)\n", "mha_chk = MultiHeadAttention(D, H)\n", "qkv = mha_chk.qkv(x_chk); q, k, v = qkv.chunk(3, dim=-1)\n", "def split(t): return t.view(B, T, H, D//H).transpose(1, 2)\n", "qh, kh, vh = map(split, (q, k, v))\n", "q.shape, qh.shape, (H * (D//H)) == D\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Residual + LayerNorm (pre-norm)\n", "class Residual(nn.Module):\n", "    def __init__(self, d_model):\n", "        super().__init__(); self.norm = nn.LayerNorm(d_model)\n", "    def forward(self, x, sublayer, *args, **kwargs):\n", "        return x + sublayer(self.norm(x), *args, **kwargs)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Feed-forward\n", "class FeedForward(nn.Module):\n", "    def __init__(self, d_model, d_ff, dropout=0.0):\n", "        super().__init__(); self.net = nn.Sequential(\n", "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n", "            nn.Linear(d_ff, d_model), nn.Dropout(dropout))\n", "    def forward(self, x): return self.net(x)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Transformer block\n", "class TransformerBlock(nn.Module):\n", "    def __init__(self, d_model, num_heads, d_ff, dropout=0.0):\n", "        super().__init__()\n", "        self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n", "        self.ffn = FeedForward(d_model, d_ff, dropout)\n", "        self.res1 = Residual(d_model); self.res2 = Residual(d_model)\n", "    def forward(self, x, mask=None):\n", "        x = self.res1(x, self.mha, mask)\n", "        x = self.res2(x, self.ffn)\n", "        return x\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create toy input\n", "B, T, D = 2, 6, 16\n", "x = torch.randn(B, T, D)\n", "x\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add sinusoidal positions\n", "pe = sinusoidal_positions(T, D)\n", "pe.shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = x + pe[None, :, :]\n", "x.shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Causal mask\n", "mask = torch.tril(torch.ones(T, T))[None, :, :]\n", "mask.shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Block and forward\n", "block = TransformerBlock(D, num_heads=4, d_ff=64, dropout=0.1)\n", "block\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = block(x, mask)\n", "y.shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualize attention weights of a single head from the block\n", "with torch.no_grad():\n", "    B, T, Dm = x.shape\n", "    # re-compute q,k for visualization\n", "    qkv = block.mha.qkv(x); q, k, v = qkv.chunk(3, dim=-1)\n", "    H = block.mha.h; Dh = block.mha.d\n", "    def split(t): return t.view(B, T, H, Dh).transpose(1, 2)\n", "    qh, kh = map(split, (q, k))\n", "    d = Dh\n", "    scores = (qh @ kh.transpose(-2, -1)) / (d ** 0.5)   # [B,H,T,T]\n", "    scores = scores.masked_fill(mask == 0, float('-inf'))\n", "    w = torch.softmax(scores, dim=-1)[0, 0]             # head 0 weights [T,T]\n", "plt.figure(figsize=(4,3))\n", "plt.imshow(w, cmap='magma', aspect='auto')\n", "plt.colorbar(label='weight')\n", "plt.xlabel('key\\npositions')\n", "plt.ylabel('query positions')\n", "plt.title('Head 0 weights (toy)')\n", "plt.tight_layout()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Second block for a quick stability check\n", "block2 = TransformerBlock(D, num_heads=4, d_ff=64, dropout=0.1)\n", "block2\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Measure mean/std before and after each block\n", "with torch.no_grad():\n", "    def stats(t): return float(t.mean()), float(t.std())\n", "    m0, s0 = stats(x)\n", "    y1 = block(x, mask)\n", "    m1, s1 = stats(y1)\n", "    y2 = block2(y1, mask)\n", "    m2, s2 = stats(y2)\n", "    print('(mean,std) before:', (round(m0,4), round(s0,4)))\n", "    print('after block 1   :', (round(m1,4), round(s1,4)))\n", "    print('after block 2   :', (round(m2,4), round(s2,4)))\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}