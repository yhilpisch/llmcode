{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Chapter 13 \u2014 Improvements & Extensions (Colab)\n\n", "Demonstrate AMP, gradient clipping, warmup+cosine, and accumulation in a\n", "tiny, self-contained loop. One creation per cell; lines kept short.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This notebook demonstrates training improvements from Chapter 13: \n", "- Mixed precision (AMP) on CUDA for speed\n", "- Gradient clipping for stability\n", "- Warmup + cosine learning-rate schedule\n", "- Gradient accumulation to emulate larger batches\n", "Each cell creates one object and shows it immediately to match the book's \n", "creation rule. Comments explain why each step matters.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Torch setup\n", "import sys, subprocess\n", "import contextlib\n", "try:\n", "    import torch  # noqa: F401\n", "except Exception:\n", "    idx = 'https://download.pytorch.org/whl/cpu'\n", "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n", "                           '--index-url', idx, 'torch'])\n", "    import torch  # noqa: F401\n", "torch.manual_seed(0); device = ('cuda' if torch.cuda.is_available() else\n", "    'cpu'); device\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Tiny language model: embedding + linear head.\n", "# Used to demonstrate mechanics without heavy compute.\n", "class TinyLM(torch.nn.Module):\n", "    def __init__(self, V=64, D=64):\n", "        \"\"\"Create a minimal LM with vocabulary V and hidden size D.\n", "        Embedding maps ids->vectors; linear head maps vectors->logits.\n", "        \"\"\"\n", "        super().__init__(); self.emb = torch.nn.Embedding(V, D)\n", "        self.lin = torch.nn.Linear(D, V)\n", "    def forward(self, x, y=None):\n", "        \"\"\"Return (logits, loss). Loss is CE if targets y are given.\n", "        \"\"\"\n", "        h = self.emb(x); logits = self.lin(h)\n", "        loss = None\n", "        if y is not None:\n", "            B,T,V = logits.shape\n", "            loss = torch.nn.functional.cross_entropy(\n", "                logits.reshape(B*T, V), y.reshape(B*T))\n", "        return logits, loss\n", "TinyLM()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Data: random ids to exercise the loop\n", "V, T, B = 64, 32, 64\n", "ids = torch.randint(0, V, (B, T))\n", "ids.shape\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Warmup + cosine schedule: scale base LR in [minr, 1].\n", "# Warmup ramps 0->1; cosine glides 1->minr over remaining steps.\n", "import math\n", "def warmup_cosine_lambda(warmup, total, minr=0.1):\n", "    \"\"\"Return a LambdaLR-compatible function.\n", "    warmup: warmup steps; total: total steps; minr: floor ratio.\n", "    \"\"\"\n", "    def f(step):\n", "        s = step + 1\n", "        if s <= warmup: return s/float(warmup)\n", "        t = s - warmup; frac = t/max(1,total-warmup)\n", "        return minr + (1-minr)*0.5*(1+math.cos(math.pi*frac))\n", "    return f\n", "warmup_cosine_lambda(10, 100)(0)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import contextlib\n", "# Train with AMP (CUDA), clipping, accumulation, and schedule.\n", "# On CPU/MPS, AMP is disabled and training runs in full precision.\n", "model = TinyLM().to(device)\n", "opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n", "sched = torch.optim.lr_scheduler.LambdaLR(\n", "    opt, warmup_cosine_lambda(10, 100, 0.1))\n", "try:\n    scaler = torch.amp.GradScaler('cuda', enabled=(device=='cuda'))\nexcept Exception:\n    scaler = torch.cuda.amp.GradScaler(enabled=(device=='cuda'))\n", "accum, clip = 4, 1.0; hist = []\n", "model.train(); opt.zero_grad(set_to_none=True)\n", "for step in range(100):\n", "    x = ids.to(device); y = ids.to(device)\n", "    # CUDA autocast if available; otherwise no-op\n", "    ac = (torch.amp.autocast('cuda', dtype=torch.float16)\n          if device=='cuda' else contextlib.nullcontext())\n", "          if device=='cuda' else contextlib.nullcontext())\n", "    with ac:\n", "        _, loss = model(x, y)\n", "    (scaler.scale(loss) if device=='cuda' else loss).backward()\n", "    if (step+1) % accum == 0:\n", "        if device=='cuda': scaler.unscale_(opt)\n", "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n", "        if device=='cuda': scaler.step(opt); scaler.update()\n", "        else: opt.step()\n", "        opt.zero_grad(set_to_none=True); sched.step()\n", "    if step % 10 == 0: hist.append(float(loss.detach().cpu().item()))\n", "hist[:5]\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}