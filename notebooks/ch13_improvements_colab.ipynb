{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6c7a4d",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78c8e6",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch \u2014 A Step-by-Step Guide Using Python and PyTorch\n",
    "## Chapter 13 \u2014 Improvements & Extensions\n",
    "**\u00a9 Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dcb0fc",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Prototype improvement ideas such as LoRA, adapters, or data augmentation.\n",
    "- Measure the trade-offs between accuracy gains and computational costs.\n",
    "- Document experiments thoroughly so you can reproduce winners later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd6b6e6",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "We experiment with several upgrade paths, each isolated so you can evaluate impact independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b976c8a",
   "metadata": {},
   "source": [
    "### Study Tips\n",
    "\n",
    "Change one variable at a time. Rapid iteration is tempting, but disciplined ablations reveal what truly matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcdd880",
   "metadata": {},
   "source": [
    "This notebook demonstrates training improvements from Chapter 13: \n",
    "- Mixed precision (AMP) on CUDA for speed\n",
    "- Gradient clipping for stability\n",
    "- Warmup + cosine learning-rate schedule\n",
    "- Gradient accumulation to emulate larger batches\n",
    "Each cell creates one object and shows it immediately to match the book's \n",
    "creation rule. Comments explain why each step matters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4a16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch setup\n",
    "import sys, subprocess\n",
    "import contextlib\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "except Exception:\n",
    "    idx = 'https://download.pytorch.org/whl/cpu'\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                           '--index-url', idx, 'torch'])\n",
    "    import torch  # noqa: F401\n",
    "torch.manual_seed(0); device = ('cuda' if torch.cuda.is_available() else\n",
    "    'cpu'); device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563cb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny language model: embedding + linear head.\n",
    "# Used to demonstrate mechanics without heavy compute.\n",
    "class TinyLM(torch.nn.Module):\n",
    "    def __init__(self, V=64, D=64):\n",
    "        \"\"\"Create a minimal LM with vocabulary V and hidden size D.\n",
    "        Embedding maps ids->vectors; linear head maps vectors->logits.\n",
    "        \"\"\"\n",
    "        super().__init__(); self.emb = torch.nn.Embedding(V, D)\n",
    "        self.lin = torch.nn.Linear(D, V)\n",
    "    def forward(self, x, y=None):\n",
    "        \"\"\"Return (logits, loss). Loss is CE if targets y are given.\n",
    "        \"\"\"\n",
    "        h = self.emb(x); logits = self.lin(h)\n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            B,T,V = logits.shape\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.reshape(B*T, V), y.reshape(B*T))\n",
    "        return logits, loss\n",
    "TinyLM()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fe05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data: random ids to exercise the loop\n",
    "V, T, B = 64, 32, 64\n",
    "ids = torch.randint(0, V, (B, T))\n",
    "ids.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05663fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup + cosine schedule: scale base LR in [minr, 1].\n",
    "# Warmup ramps 0->1; cosine glides 1->minr over remaining steps.\n",
    "import math\n",
    "def warmup_cosine_lambda(warmup, total, minr=0.1):\n",
    "    \"\"\"Return a LambdaLR-compatible function.\n",
    "    warmup: warmup steps; total: total steps; minr: floor ratio.\n",
    "    \"\"\"\n",
    "    def f(step):\n",
    "        s = step + 1\n",
    "        if s <= warmup: return s/float(warmup)\n",
    "        t = s - warmup; frac = t/max(1,total-warmup)\n",
    "        return minr + (1-minr)*0.5*(1+math.cos(math.pi*frac))\n",
    "    return f\n",
    "warmup_cosine_lambda(10, 100)(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b50f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "# Train with AMP (CUDA), clipping, accumulation, and schedule.\n",
    "# On CPU/MPS, AMP is disabled and training runs in full precision.\n",
    "model = TinyLM().to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "sched = torch.optim.lr_scheduler.LambdaLR(\n",
    "    opt, warmup_cosine_lambda(10, 100, 0.1))\n",
    "try:\n",
    "    scaler = torch.amp.GradScaler('cuda', enabled=(device == 'cuda'))\n",
    "except Exception:\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n",
    "accum, clip = 4, 1.0\n",
    "hist = []\n",
    "model.train()\n",
    "opt.zero_grad(set_to_none=True)\n",
    "for step in range(100):\n",
    "    x = ids.to(device)\n",
    "    y = ids.to(device)\n",
    "    autocast_ctx = (\n",
    "        torch.amp.autocast('cuda', dtype=torch.float16)\n",
    "        if device == 'cuda' else contextlib.nullcontext()\n",
    "    )\n",
    "    with autocast_ctx:\n",
    "        _, loss = model(x, y)\n",
    "    if device == 'cuda':\n",
    "        scaler.scale(loss).backward()\n",
    "    else:\n",
    "        loss.backward()\n",
    "    if (step + 1) % accum == 0:\n",
    "        if device == 'cuda':\n",
    "            scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        if device == 'cuda':\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            opt.step()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        sched.step()\n",
    "    if step % 10 == 0:\n",
    "        hist.append(float(loss.detach().cpu().item()))\n",
    "hist[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9384616",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Implement LoRA for one transformer layer and compare training speed.\n",
    "- Try a data augmentation technique and report its effect on validation metrics.\n",
    "- Create a decision matrix that scores each extension by cost, complexity, and expected impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d12000",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}