{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30306def",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15596e4d",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch \u2014 A Step-by-Step Guide Using Python and PyTorch\n",
    "## Chapter 6 \u2014 From Words to Vectors\n",
    "**\u00a9 Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca1dc67",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Build intuition for distributional semantics with co-occurrence counts and simple embeddings.\n",
    "- Visualize learned vectors to validate that geometry matches linguistic expectations.\n",
    "- Evaluate embedding quality with quick intrinsic metrics before using them downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187241c3",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "You will progress from count-based representations to learned embeddings, finishing with visual diagnostics that highlight semantic structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8526e4de",
   "metadata": {},
   "source": [
    "### Study Tips\n",
    "\n",
    "Keep the plots visible as you tweak hyperparameters. Subtle changes in window size or negative sampling show up immediately in the geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57165af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure torch is available (Colab friendly)\n",
    "try:\n",
    "    import torch  # noqa\n",
    "    print('torch:', torch.__version__)\n",
    "except Exception:\n",
    "    import os\n",
    "    gpu = os.system('nvidia-smi > /dev/null 2>&1') == 0\n",
    "    index = (\n",
    "        'https://download.pytorch.org/whl/cu121'\n",
    "        if gpu else 'https://download.pytorch.org/whl/cpu'\n",
    "    )\n",
    "    get_ipython().run_line_magic('pip', f'install -q torch --index-url {index}')\n",
    "    import torch\n",
    "    print('torch:', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383b4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tiny corpus (use data/philosophy.txt if you have it)\n",
    "from pathlib import Path\n",
    "text_path = Path('mini.txt')\n",
    "if not text_path.exists():\n",
    "    text_path.write_text('Hello world. Hello vectors.', encoding='utf-8')\n",
    "text_path, text_path.read_text()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94034eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, List, Dict\n",
    "\n",
    "@dataclass\n",
    "class Vocab:\n",
    "    token_to_id: Dict[str, int]\n",
    "    id_to_token: List[str]\n",
    "    pad: int\n",
    "    unk: int\n",
    "    @classmethod\n",
    "    def build(\n",
    "        cls,\n",
    "        tokens: Iterable[str],\n",
    "        min_freq: int = 1,\n",
    "        specials: Iterable[str] = ('<PAD>', '<UNK>'),\n",
    "    ) -> 'Vocab':\n",
    "        counter = Counter(tokens)\n",
    "        id_to_token = list(specials)\n",
    "        for tok, freq in counter.most_common():\n",
    "            if freq >= min_freq and tok not in id_to_token:\n",
    "                id_to_token.append(tok)\n",
    "        token_to_id = {t: i for i, t in enumerate(id_to_token)}\n",
    "        pad = token_to_id[specials[0]]\n",
    "        unk = token_to_id[specials[1]]\n",
    "        return Vocab(token_to_id, id_to_token, pad, unk)\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.id_to_token)\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    \"\"\"Tiny tokenizer for chapter 6 (char or word level).\"\"\"\n",
    "    def __init__(self, vocab: Vocab, level: str = 'char') -> None:\n",
    "        assert level in {'char','word'}\n",
    "        self.vocab = vocab\n",
    "        self.level = level\n",
    "        self.pad = vocab.pad\n",
    "        self.unk = vocab.unk\n",
    "    @staticmethod\n",
    "    def _split(text: str, level: str) -> List[str]:\n",
    "        if level == 'char':\n",
    "            return list(text)\n",
    "        out: List[str] = []\n",
    "        token: List[str] = []\n",
    "        for ch in text:\n",
    "            if ch.isalnum():\n",
    "                token.append(ch.lower())\n",
    "            else:\n",
    "                if token:\n",
    "                    out.append(''.join(token)); token = []\n",
    "                if ch.strip():\n",
    "    def from_file(\n",
    "        cls,\n",
    "        path: str | Path,\n",
    "        level: str = 'char',\n",
    "        min_freq: int = 1,\n",
    "    ) -> 'SimpleTokenizer':\n",
    "        ids = []\n",
    "        for tok in self._split(text, self.level):\n",
    "            ids.append(self.vocab.token_to_id.get(tok, self.unk))\n",
    "        return ids\n",
    "            out.append(''.join(token))\n",
    "        return out\n",
    "    @classmethod\n",
    "    def from_file(\n",
    "        cls,\n",
    "        path: str | Path,\n",
    "        level: str = 'char',\n",
    "        min_freq: int = 1,\n",
    "    ) -> 'SimpleTokenizer':\n",
    "        ids = []\n",
    "        for tok in self._split(text, self.level):\n",
    "            ids.append(self.vocab.token_to_id.get(tok, self.unk))\n",
    "        return ids\n",
    "        tokens = cls._split(text, level)\n",
    "        vocab = Vocab.build(tokens, min_freq=min_freq)\n",
    "        return cls(vocab=vocab, level=level)\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        ids = []\n",
    "        for tok in self._split(text, self.level):\n",
    "            ids.append(self.vocab.token_to_id.get(tok, self.unk))\n",
    "        return ids\n",
    "    def decode(self, ids: Iterable[int]) -> str:\n",
    "        toks: List[str] = []\n",
    "        for i in ids:\n",
    "            if 0 <= i < len(self.vocab.id_to_token):\n",
    "                tok = self.vocab.id_to_token[i]\n",
    "                if tok not in {'<PAD>','<UNK>'}:\n",
    "                    toks.append(tok)\n",
    "            else:\n",
    "                toks.append('<UNK>')\n",
    "        if self.level == 'char':\n",
    "            return ''.join(toks)\n",
    "        out: List[str] = []\n",
    "        for t in toks:\n",
    "            if not out: out.append(t)\n",
    "            elif t.isalnum(): out.append(' ' + t)\n",
    "            else: out.append(t)\n",
    "        return ''.join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d68e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a character-level tokenizer using the class above\n",
    "tok = SimpleTokenizer.from_file(str(text_path), level='char')\n",
    "len(tok.vocab), list(tok.vocab.token_to_id.items())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4446264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure `tok` exists (create if missing)\n",
    "try:\n",
    "    tok\n",
    "except NameError:\n",
    "    tok = SimpleTokenizer.from_file(str(text_path), level='char')\n",
    "# Encode and decode\n",
    "ids = tok.encode('Hello world.')\n",
    "decoded = tok.decode(ids)\n",
    "ids, decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding table\n",
    "E = torch.nn.Embedding(num_embeddings=len(tok.vocab), embedding_dim=8)\n",
    "E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a small batch of token ids\n",
    "batch = [tok.encode('Hello'), tok.encode('vectors')]\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048a26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad to equal length (PAD=0)\n",
    "P = tok.pad\n",
    "P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f5a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = max(len(x) for x in batch)\n",
    "lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a772d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([s + [P]*(lens-len(s)) for s in batch])\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e883bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lookup embeddings\n",
    "E(x).shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7001aa0",
   "metadata": {},
   "source": [
    "## Word\u2011Level Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SimpleTokenizer defined in a previous cell\n",
    "tok_w = SimpleTokenizer.from_file(str(text_path), level='word')\n",
    "len(tok.vocab), len(tok_w.vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_w.encode('Hello vectors.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4581b41",
   "metadata": {},
   "source": [
    "## Padding Strategies and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = tok.pad\n",
    "P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be166a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [tok.encode('Hello'), tok.encode('vectors')]\n",
    "batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f31d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = max(len(s) for s in batch)\n",
    "L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fdb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_pad = [s + [P]*(L-len(s)) for s in batch]\n",
    "right_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bcff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_pad  = [[P]*(L-len(s)) + s for s in batch]\n",
    "left_pad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb6edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor(right_pad)\n",
    "pad_mask = (x != P).float()\n",
    "T = x.size(1)\n",
    "causal = torch.tril(torch.ones(T, T))\n",
    "combined = pad_mask[:, None, :] * causal\n",
    "pad_mask.shape, causal.shape, combined.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65873842",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Train embeddings on a new corpus and compare cosine similarities for a handful of anchor words.\n",
    "- Experiment with dimensionalities of 16, 64, and 128 to see how expressiveness and overfitting trade off.\n",
    "- Add an intrinsic evaluation metric (analogy completion or clustering) and report the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582112aa",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}