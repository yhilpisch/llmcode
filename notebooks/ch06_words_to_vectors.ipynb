{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Chapter 6 \u2014 From Words to Vectors\n\n", "Self\u2011contained tokenization + embeddings. Works locally or on Colab."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import matplotlib.pyplot as plt\n", "plt.style.use('seaborn-v0_8')\n", "%config InlineBackend.figure_format = 'svg'\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ensure torch is available (Colab friendly)\n", "try:\n", "    import torch  # noqa\n", "    print('torch:', torch.__version__)\n", "except Exception:\n", "    import os\n", "    gpu = os.system('nvidia-smi > /dev/null 2>&1') == 0\n", "    index = 'https://download.pytorch.org/whl/cu121' if gpu else 'https://download.pytorch.org/whl/cpu'\n", "    get_ipython().run_line_magic('pip', f'install -q torch --index-url {index}')\n", "    import torch\n", "    print('torch:', torch.__version__)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# A tiny corpus (use data/philosophy.txt if you have it)\n", "from pathlib import Path\n", "text_path = Path('mini.txt')\n", "if not text_path.exists():\n", "    text_path.write_text('Hello world. Hello vectors.', encoding='utf-8')\n", "text_path, text_path.read_text()\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import annotations\nfrom collections import Counter\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Iterable, List, Dict\n\n@dataclass\nclass Vocab:\n    token_to_id: Dict[str, int]\n    id_to_token: List[str]\n    pad: int\n    unk: int\n    @classmethod\n    def build(cls, tokens: Iterable[str], min_freq: int = 1, specials: Iterable[str] = (\"<PAD>\", \"<UNK>\")) -> \"Vocab\":\n        counter = Counter(tokens)\n        id_to_token = list(specials)\n        for tok, freq in counter.most_common():\n            if freq >= min_freq and tok not in id_to_token:\n                id_to_token.append(tok)\n        token_to_id = {t: i for i, t in enumerate(id_to_token)}\n        pad = token_to_id[specials[0]]\n        unk = token_to_id[specials[1]]\n        return Vocab(token_to_id, id_to_token, pad, unk)\n    def __len__(self) -> int:\n        return len(self.id_to_token)\n\nclass SimpleTokenizer:\n    \"\"\"Tiny tokenizer for chapter 6 (char or word level).\"\"\"\n    def __init__(self, vocab: Vocab, level: str = 'char') -> None:\n        assert level in {'char','word'}\n        self.vocab = vocab\n        self.level = level\n        self.pad = vocab.pad\n        self.unk = vocab.unk\n    @staticmethod\n    def _split(text: str, level: str) -> List[str]:\n        if level == 'char':\n            return list(text)\n        out: List[str] = []\n        token: List[str] = []\n        for ch in text:\n            if ch.isalnum():\n                token.append(ch.lower())\n            else:\n                if token:\n                    out.append(''.join(token)); token = []\n                if ch.strip():\n                    out.append(ch)\n        if token:\n            out.append(''.join(token))\n        return out\n    @classmethod\n    def from_file(cls, path: str | Path, level: str = 'char', min_freq: int = 1) -> 'SimpleTokenizer':\n        text = Path(path).read_text(encoding='utf-8')\n        tokens = cls._split(text, level)\n        vocab = Vocab.build(tokens, min_freq=min_freq)\n        return cls(vocab=vocab, level=level)\n    def encode(self, text: str) -> List[int]:\n        return [self.vocab.token_to_id.get(tok, self.unk) for tok in self._split(text, self.level)]\n    def decode(self, ids: Iterable[int]) -> str:\n        toks: List[str] = []\n        for i in ids:\n            if 0 <= i < len(self.vocab.id_to_token):\n                tok = self.vocab.id_to_token[i]\n                if tok not in {'<PAD>','<UNK>'}:\n                    toks.append(tok)\n            else:\n                toks.append('<UNK>')\n        if self.level == 'char':\n            return ''.join(toks)\n        out: List[str] = []\n        for t in toks:\n            if not out: out.append(t)\n            elif t.isalnum(): out.append(' ' + t)\n            else: out.append(t)\n        return ''.join(out)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build a character-level tokenizer using the class above\n", "tok = SimpleTokenizer.from_file(str(text_path), level='char')\n", "len(tok.vocab), list(tok.vocab.token_to_id.items())[:10]\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Ensure `tok` exists (create if missing)\n", "try:\n", "    tok\n", "except NameError:\n", "    tok = SimpleTokenizer.from_file(str(text_path), level='char')\n", "# Encode and decode\n", "ids = tok.encode('Hello world.')\n", "decoded = tok.decode(ids)\n", "ids, decoded\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Embedding table\n", "E = torch.nn.Embedding(num_embeddings=len(tok.vocab), embedding_dim=8)\n", "E\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build a small batch of token ids\n", "batch = [tok.encode('Hello'), tok.encode('vectors')]\n", "batch\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Pad to equal length (PAD=0)\n", "P = tok.pad\n", "P\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["lens = max(len(x) for x in batch)\n", "lens\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["x = torch.tensor([s + [P]*(lens-len(s)) for s in batch])\n", "x\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Lookup embeddings\n", "E(x).shape\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Word\u2011Level Example"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Using SimpleTokenizer defined in a previous cell\n", "tok_w = SimpleTokenizer.from_file(str(text_path), level='word')\n", "len(tok.vocab), len(tok_w.vocab)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["tok_w.encode('Hello vectors.')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Padding Strategies and Masks"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["P = tok.pad\n", "P\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch = [tok.encode('Hello'), tok.encode('vectors')]\n", "batch\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["L = max(len(s) for s in batch)\n", "L\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["right_pad = [s + [P]*(L-len(s)) for s in batch]\n", "right_pad\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["left_pad  = [[P]*(L-len(s)) + s for s in batch]\n", "left_pad\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "x = torch.tensor(right_pad)\n", "pad_mask = (x != P).float()\n", "T = x.size(1)\n", "causal = torch.tril(torch.ones(T, T))\n", "combined = pad_mask[:, None, :] * causal\n", "pad_mask.shape, causal.shape, combined.shape\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}