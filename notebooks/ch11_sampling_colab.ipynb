{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80e4cff1",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da8bfe",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch — A Step-by-Step Guide Using Python and PyTorch\n",
    "## Chapter 11 — Testing & Sampling\n",
    "**© Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438995f",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Compare sampling strategies such as greedy, top-k, and nucleus sampling.\n",
    "- Evaluate qualitative outputs with checklists anchored to your use case.\n",
    "- Instrument temperature sweeps to understand controllability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e684d84",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "We load a trained checkpoint, generate continuations with multiple decoding schemes, and analyze the trade-offs each introduces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ed00b",
   "metadata": {},
   "source": [
    "### Study Tips\n",
    "\n",
    "Save representative generations for later review. Side-by-side comparisons are invaluable during stakeholder discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207f0121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch + plotting setup\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "except Exception:\n",
    "    idx = 'https://download.pytorch.org/whl/cpu'\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n",
    "                           '--index-url', idx, 'torch', 'torchvision',\n",
    "                           'torchaudio'])\n",
    "    import torch  # noqa: F401\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "torch.manual_seed(0); 'ok'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081ca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability at temperature T for toy logits.\n",
    "logits = torch.tensor([[2.0, 1.0, 0.2, -1.0]])\n",
    "\n",
    "def probs_at_T(T):\n",
    "    \"\"\"Return softmax(logits/T) for a single toy row.\n",
    "    Lower T sharpens, higher T flattens.\n",
    "    \"\"\"\n",
    "    p = torch.softmax(logits / T, dim=-1)\n",
    "    return p\n",
    "probs_at_T(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9421af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot temperature effects\n",
    "Ts = [0.7, 1.0, 1.3]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(6.0, 2.2), constrained_layout=True)\n",
    "for ax, T in zip(axes, Ts):\n",
    "    p = probs_at_T(T)[0]\n",
    "    ax.bar(range(len(p)), p, color='#0A66C2')\n",
    "    ax.set_title(f'T={T}')\n",
    "    ax.set_ylim(0, 1.0); ax.set_xticks([]); ax.set_yticks([])\n",
    "fig.suptitle('Temperature'); fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top-k and top-p filters: set low-prob tokens to a large negative\n",
    "# logit so softmax effectively assigns zero probability.\n",
    "def top_k_filter(logits, k):\n",
    "    \"\"\"Keep only the k largest logits per row.\n",
    "    Others are set to a very negative number.\n",
    "    \"\"\"\n",
    "    if k <= 0: return logits\n",
    "    v, _ = torch.topk(logits, k)\n",
    "    thr = v[:, [-1]]\n",
    "    return torch.where(logits < thr, torch.tensor(-1e9), logits)\n",
    "def top_p_filter(logits, p):\n",
    "    \"\"\"Keep the smallest set of tokens whose cumulative\n",
    "    probability exceeds p. Works row-wise.\n",
    "    \"\"\"\n",
    "    if p <= 0 or p >= 1: return logits\n",
    "    s, idx = torch.sort(logits, dim=-1, descending=True)\n",
    "    pr = torch.softmax(s, dim=-1)\n",
    "    cum = torch.cumsum(pr, dim=-1)\n",
    "    mask = cum > p; mask[..., 0] = False\n",
    "    s = s.masked_fill(mask, -1e9)\n",
    "    out = torch.empty_like(s).scatter_(1, idx, s)\n",
    "    return out\n",
    "top_k_filter(logits, 3), top_p_filter(logits, 0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe19648c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One sampling step on toy logits: apply temperature and optional\n",
    "# top-k/top-p, then draw the next token (or greedy if T<=0).\n",
    "def step_sample(logits, T=1.0, k=None, p=None):\n",
    "    \"\"\"Return next token ids for a single step.\n",
    "    \"\"\"\n",
    "    x = logits / T if T > 0 else logits\n",
    "    if k is not None: x = top_k_filter(x, k)\n",
    "    if p is not None: x = top_p_filter(x, p)\n",
    "    if T <= 0: return torch.argmax(x, dim=-1, keepdim=True)\n",
    "    pr = torch.softmax(x, dim=-1)\n",
    "    return torch.multinomial(pr, num_samples=1)\n",
    "step_sample(logits, T=0.8, k=3, p=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dbd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple dummy language model for a quick perplexity demo.\n",
    "class DummyLM(torch.nn.Module):\n",
    "    def __init__(self, V):\n",
    "        super().__init__(); self.V = V\n",
    "    def forward(self, x, targets=None):\n",
    "        B, T = x.size(); logits = torch.zeros(B, T, self.V)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits.reshape(B*T, self.V), targets.reshape(B*T)\n",
    "            )\n",
    "        return logits, loss\n",
    "def perplexity(model, loader):\n",
    "    \"\"\"Compute (H, exp(H)) over a loader of (x,y) pairs.\n",
    "    \"\"\"\n",
    "    total, tokens = 0.0, 0\n",
    "    for x, y in loader:\n",
    "        _, loss = model(x, targets=y)\n",
    "        total += float(loss.detach().item()) * y.numel()\n",
    "        tokens += int(y.numel())\n",
    "    H = total / max(1, tokens)\n",
    "    import math; return H, math.exp(H)\n",
    "V = 16; model = DummyLM(V)\n",
    "ids = torch.randint(0, V, (1, 128))\n",
    "class DS(torch.utils.data.Dataset):\n",
    "    def __len__(self): return 64\n",
    "    def __getitem__(self, i):\n",
    "        x = ids[0, i:i+32]; y = ids[0, i+1:i+33]; return x, y\n",
    "dl = torch.utils.data.DataLoader(DS(), batch_size=16, drop_last=True)\n",
    "perplexity(model, dl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81391417",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Implement beam search and compare its outputs against nucleus sampling.\n",
    "- Add automated toxicity or bias checks using an available open-source detector.\n",
    "- Create a table summarizing how temperature and top-k interact across several prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0842cb66",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
