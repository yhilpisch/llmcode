{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Chapter 11 \u2014 Testing & Sampling (Colab)\n\n", "Explore perplexity and decoding: greedy, temperature, top\u2011k, top\u2011p. One\n", "creation per cell; show each object or result immediately.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Torch + plotting setup\n", "import sys, subprocess\n", "try:\n", "    import torch  # noqa: F401\n", "except Exception:\n", "    idx = 'https://download.pytorch.org/whl/cpu'\n", "    subprocess.check_call([sys.executable, '-m', 'pip', 'install',\n", "                           '--index-url', idx, 'torch', 'torchvision',\n", "                           'torchaudio'])\n", "    import torch  # noqa: F401\n", "import matplotlib.pyplot as plt\n", "plt.style.use('seaborn-v0_8')\n", "%config InlineBackend.figure_format = 'svg'\n", "torch.manual_seed(0); 'ok'\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Probability at temperature T for toy logits.\n", "def probs_at_T(T):\n", "    \"\"\"Return softmax(logits/T) for a single toy row.\n", "    Lower T sharpens, higher T flattens.\n", "    \"\"\"\n", "    p = torch.softmax(logits / T, dim=-1)\n", "    return p\n", "probs_at_T(1.0)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Plot temperature effects\n", "Ts = [0.7, 1.0, 1.3]\n", "fig, axes = plt.subplots(1, 3, figsize=(6.0, 2.2), constrained_layout=True)\n", "for ax, T in zip(axes, Ts):\n", "    p = probs_at_T(T)[0]\n", "    ax.bar(range(len(p)), p, color='#0A66C2')\n", "    ax.set_title(f'T={T}')\n", "    ax.set_ylim(0, 1.0); ax.set_xticks([]); ax.set_yticks([])\n", "fig.suptitle('Temperature'); fig\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Top-k and top-p filters: set low-prob tokens to a large negative\n", "# logit so softmax effectively assigns zero probability.\n", "def top_k_filter(logits, k):\n", "    \"\"\"Keep only the k largest logits per row.\n", "    Others are set to a very negative number.\n", "    \"\"\"\n", "    if k <= 0: return logits\n", "    v, _ = torch.topk(logits, k)\n", "    thr = v[:, [-1]]\n", "    return torch.where(logits < thr, torch.tensor(-1e9), logits)\n", "def top_p_filter(logits, p):\n", "    \"\"\"Keep the smallest set of tokens whose cumulative\n", "    probability exceeds p. Works row-wise.\n", "    \"\"\"\n", "    if p <= 0 or p >= 1: return logits\n", "    s, idx = torch.sort(logits, dim=-1, descending=True)\n", "    pr = torch.softmax(s, dim=-1)\n", "    cum = torch.cumsum(pr, dim=-1)\n", "    mask = cum > p; mask[..., 0] = False\n", "    s = s.masked_fill(mask, -1e9)\n", "    out = torch.empty_like(s).scatter_(1, idx, s)\n", "    return out\n", "top_k_filter(logits, 3), top_p_filter(logits, 0.9)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# One sampling step on toy logits: apply temperature and optional\n", "# top-k/top-p, then draw the next token (or greedy if T<=0).\n", "def step_sample(logits, T=1.0, k=None, p=None):\n", "    \"\"\"Return next token ids for a single step.\n", "    \"\"\"\n", "    x = logits / T if T > 0 else logits\n", "    if k is not None: x = top_k_filter(x, k)\n", "    if p is not None: x = top_p_filter(x, p)\n", "    if T <= 0: return torch.argmax(x, dim=-1, keepdim=True)\n", "    pr = torch.softmax(x, dim=-1)\n", "    return torch.multinomial(pr, num_samples=1)\n", "step_sample(logits, T=0.8, k=3, p=0.9)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Simple dummy language model for a quick perplexity demo.\n", "class DummyLM(torch.nn.Module):\n", "    def __init__(self, V):\n", "        super().__init__(); self.V = V\n", "    def forward(self, x, targets=None):\n", "        B, T = x.size(); logits = torch.zeros(B, T, self.V)\n", "        loss = None\n", "        if targets is not None:\n", "            loss = torch.nn.functional.cross_entropy(\n", "                logits.reshape(B*T, self.V), targets.reshape(B*T)\n", "            )\n", "        return logits, loss\n", "def perplexity(model, loader):\n", "    \"\"\"Compute (H, exp(H)) over a loader of (x,y) pairs.\n", "    \"\"\"\n", "    total, tokens = 0.0, 0\n", "    for x, y in loader:\n", "        _, loss = model(x, targets=y)\n", "        total += float(loss.detach().item()) * y.numel()\n", "        tokens += int(y.numel())\n", "    H = total / max(1, tokens)\n", "    import math; return H, math.exp(H)\n", "V = 16; model = DummyLM(V)\n", "ids = torch.randint(0, V, (1, 128))\n", "class DS(torch.utils.data.Dataset):\n", "    def __len__(self): return 64\n", "    def __getitem__(self, i):\n", "        x = ids[0, i:i+32]; y = ids[0, i+1:i+33]; return x, y\n", "dl = torch.utils.data.DataLoader(DS(), batch_size=16, drop_last=True)\n", "perplexity(model, dl)\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}