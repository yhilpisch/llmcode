{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d720f282",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8ea2e",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch — A Step-by-Step Guide Using Python and PyTorch\n",
    "## Chapter 4 — Hardware Platforms & Software Setup\n",
    "**© Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0eca6b",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Inspect the hardware Colab assigned (CPU, GPU, RAM) and log the results.\n",
    "- Benchmark lightweight operations to estimate the runtime of later experiments.\n",
    "- Decide when to scale up or down based on the metrics you collect here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696cebf7",
   "metadata": {},
   "source": [
    "### Hardware Discovery\n",
    "\n",
    "Understanding what resources you have prevents surprises when training longer experiments. The cells below gather GPU, CPU, and RAM details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d413986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(f\"Python      : {platform.python_version()}\")\n",
    "print(f\"Machine     : {platform.machine()}\")\n",
    "print(f\"Processor   : {platform.processor() or 'N/A'}\")\n",
    "print(f\"Logical CPU : {psutil.cpu_count(logical=True)}\")\n",
    "print(f\"Physical CPU: {psutil.cpu_count(logical=False)}\")\n",
    "print(f\"Total RAM   : {psutil.virtual_memory().total / 1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f06e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU diagnostics (works when a GPU runtime is enabled in Colab)\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_index = 0\n",
    "        props = torch.cuda.get_device_properties(gpu_index)\n",
    "        print(f\"Device      : {props.name}\")\n",
    "        print(f\"Memory (GB) : {props.total_memory / 1e9:.2f}\")\n",
    "        print(f\"SM count    : {props.multi_processor_count}\")\n",
    "    else:\n",
    "        print(\"No CUDA-capable GPU detected. Switch to a GPU runtime if needed.\")\n",
    "except Exception as exc:\n",
    "    print(f\"Torch not available or failed to query CUDA: {exc}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5432d2c",
   "metadata": {},
   "source": [
    "### Quick Benchmark\n",
    "\n",
    "Run a tiny matmul benchmark. The absolute number is less important than the comparison you can make when switching runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb72ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def benchmark_matmul(dim: int = 2048, repeats: int = 5):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    x = torch.randn(dim, dim, device=device)\n",
    "    y = torch.randn(dim, dim, device=device)\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.synchronize()\n",
    "    timings = []\n",
    "    for _ in range(repeats):\n",
    "        start = time.time()\n",
    "        _ = x @ y\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        timings.append(time.time() - start)\n",
    "    return device, sum(timings) / len(timings)\n",
    "\n",
    "device, avg = benchmark_matmul()\n",
    "print(f\"Average matmul time on {device.upper()}: {avg:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6777f19",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "Compare the metrics above with the recommendations in the chapter. Record whether your current environment is sufficient or if you should upgrade for compute-heavy chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6965f0d2",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Measure the impact of different batch sizes on the benchmark timing above.\n",
    "- Record GPU memory usage before and after running the benchmark using `torch.cuda.memory_allocated()`.\n",
    "- Summarize in a short paragraph what hardware is ideal for full training runs versus experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f87ed5",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Chapter 04 · Hardware & Software"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
