{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf14c08",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c4bacb",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch \u2014 A Step-by-Step Guide Using Python and PyTorch\n",
    "## Project Companion \u2014 attoLLM Colab Starter\n",
    "**\u00a9 Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b132250",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Sanity-check GPU, disk, and Python versions before diving into model training.\n",
    "- Mount or sync the storage location you plan to use for datasets and checkpoints.\n",
    "- Bookmark the helper utilities you expect to reuse across multiple chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6be911",
   "metadata": {},
   "source": [
    "### Starter Checklist\n",
    "\n",
    "Use the diagnostics cells here before every new Colab session. Consistent environments remove a whole class of hard-to-track bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93519a54",
   "metadata": {},
   "source": [
    "### Collaboration Notes\n",
    "\n",
    "Share a copy of this notebook with teammates so everyone runs the same validation steps before kicking off workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ace6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi || echo 'No NVIDIA GPU available'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18d930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a recent CUDA wheel when GPU is present; fallback to CPU wheel\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "def ensure_torch():\n",
    "    try:\n",
    "        import torch  # type: ignore\n",
    "        return torch\n",
    "    except Exception as exc:  # pragma: no cover - diagnostics\n",
    "        print('torch import failed:', exc)\n",
    "        if os.environ.get('COLAB_RELEASE_TAG'):\n",
    "            gpu_available = os.system('nvidia-smi > /dev/null 2>&1') == 0\n",
    "            index = (\n",
    "            subprocess.run(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                 'torch', '--index-url', index],\n",
    "                check=True,\n",
    "            )\n",
    "                if gpu_available else 'https://download.pytorch.org/whl/cpu'\n",
    "            )\n",
    "            subprocess.run(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q',\n",
    "                 'torch', '--index-url', index],\n",
    "                check=True,\n",
    "            )\n",
    "    print(\n",
    "        'Install torch manually when you have network access '\n",
    "        '(skipped for validation).'\n",
    "    )\n",
    "            return torch\n",
    "        return None\n",
    "    print(\n",
    "        'Install torch manually when you have network access '\n",
    "        '(skipped for validation).'\n",
    "    )\n",
    "\n",
    "torch = ensure_torch()\n",
    "if torch is None:\n",
    "    print(\n",
    "        'Install torch manually when you have network access '\n",
    "        '(skipped for validation).'\n",
    "    )\n",
    "else:\n",
    "    print('torch:', torch.__version__, 'cuda?', torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaffold attoLLM in /content/attollm (Colab) or a local folder otherwise\n",
    "import os\n",
    "import pathlib\n",
    "import subprocess\n",
    "import sys\n",
    "import textwrap\n",
    "\n",
    "root = (\n",
    "    pathlib.Path('/content/attollm')\n",
    "    if os.environ.get('COLAB_RELEASE_TAG')\n",
    "for d in [\n",
    "    'scripts',\n",
    "    'configs',\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'data/cache',\n",
    "    'checkpoints',\n",
    "    'tests',\n",
    "]:\n",
    ")\n",
    "\n",
    "(root / 'src/attollm').mkdir(parents=True, exist_ok=True)\n",
    "for d in [\n",
    "    'scripts',\n",
    "    'configs',\n",
    "    'data/raw',\n",
    "    'data/processed',\n",
    "    'data/cache',\n",
    "    'checkpoints',\n",
    "    'tests',\n",
    "]:\n",
    "    (root / d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(root / 'README.md').write_text('# attoLLM (Colab)\\n')\n",
    "\n",
    "gitignore_text = '\\n'.join([\n",
    "    '__pycache__/',\n",
    "    '*.pyc',\n",
    "    'data/cache/',\n",
    "    'checkpoints/',\n",
    "    'runs/',\n",
    "    '*.pt',\n",
    "    '*.pth',\n",
    "]) + '\\n'\n",
    "(root / '.gitignore').write_text(gitignore_text)\n",
    "\n",
    "requirements_text = 'numpy>=1.24\\n' 'tqdm>=4.66\\n'\n",
    "(root / 'requirements.txt').write_text(requirements_text)\n",
    "\n",
    "pyproject_text = textwrap.dedent(\"\"\"\n",
    "    [build-system]\n",
    "    requires = [\"setuptools>=68\", \"wheel\"]\n",
    "    build-backend = \"setuptools.build_meta\"\n",
    "\n",
    "    [project]\n",
    "    name = \"attollm\"\n",
    "    version = \"0.0.1\"\n",
    "    requires-python = \">=3.10\"\n",
    "    dependencies = []\n",
    "\n",
    "    [tool.setuptools]\n",
    "    package-dir = {\"\" = \"src\"}\n",
    "\n",
    "    [tool.setuptools.packages.find]\n",
    "    where = [\"src\"]\n",
    "\"\"\")\n",
    "(root / 'pyproject.toml').write_text(pyproject_text)\n",
    "\n",
    "(root / 'src/attollm/__init__.py').write_text('__all__ = [\"hello\"]\\n')\n",
    "\n",
    "    subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', '-q', '-e', str(root)],\n",
    "        check=True,\n",
    "    )\n",
    "    def main():\n",
    "        print(\"Hello from attoLLM (Colab)!\")\n",
    "\n",
    "    subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', '-q', '-e', str(root)],\n",
    "        check=True,\n",
    "    )\n",
    "        main()\n",
    "\"\"\")\n",
    "(root / 'src/attollm/hello.py').write_text(hello_py)\n",
    "\n",
    "print('Scaffolded at:', root)\n",
    "\n",
    "if os.environ.get('COLAB_RELEASE_TAG'):\n",
    "    subprocess.run(\n",
    "        [sys.executable, '-m', 'pip', 'install', '-q', '-e', str(root)],\n",
    "        check=True,\n",
    "    )\n",
    "    subprocess.run([sys.executable, '-m', 'attollm.hello'], check=True)\n",
    "else:\n",
    "    print('Running outside Colab; editable install skipped to keep validation fast.')\n",
    "    print('Inspect the scaffolded package locally at', root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b12701",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Adapt the environment check to alert you when CUDA is missing or mismatched.\n",
    "- Mount your preferred cloud storage (Drive, S3 via fsspec, etc.) and verify read/write access.\n",
    "- Create a short markdown playbook describing how you spin up a fresh Colab runtime for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f075006",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}