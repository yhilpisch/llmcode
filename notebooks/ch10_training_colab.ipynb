{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16b2cba4",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd24ed",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch — A Step-by-Step Guide Using Python and PyTorch\n",
    "## Chapter 10 — Training the Model\n",
    "**© Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f6592b",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Set up data loaders, optimizers, and schedulers for efficient training.\n",
    "- Track metrics and checkpoints so experiments are repeatable.\n",
    "- Stress-test the training loop with gradient accumulation and mixed precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfab8351",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "We wire the dataset, define the training loop, add evaluation hooks, and finish with logging utilities you can reuse elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105aa4ec",
   "metadata": {},
   "source": [
    "### Study Tips\n",
    "\n",
    "Treat this notebook like a lab notebook: record hyperparameters, seeds, and observations directly alongside the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5edc35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure PyTorch is available (CPU or CUDA)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import torch  # noqa: F401\n",
    "    print('PyTorch found')\n",
    "except Exception:\n",
    "    print('Installing PyTorch...')\n",
    "    has_cuda = False\n",
    "    try:\n",
    "        r = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        has_cuda = (r.returncode == 0)\n",
    "    except Exception:\n",
    "        has_cuda = False\n",
    "    index_url = 'https://download.pytorch.org/whl/cu121' if has_cuda else 'https://download.pytorch.org/whl/cpu'\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--index-url', index_url, 'torch', 'torchvision', 'torchaudio'])\n",
    "    import torch  # noqa: F401\n",
    "print('torch', torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d3d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports, style, and device helper\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "torch.manual_seed(0)\n",
    "device = (\n",
    "    'cuda' if torch.cuda.is_available() else\n",
    "    'mps' if getattr(torch.backends, 'mps', None)\n",
    "            and torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd42f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal tokenizer fallback: byte-level ids\n",
    "def build_ids_byte_level(text: str):\n",
    "    data = text.encode('utf-8', errors='ignore')\n",
    "    ids = torch.tensor(list(data), dtype=torch.long)\n",
    "    return ids, 256\n",
    "\n",
    "txt = ('Hello world.\\nHello vectors.\\n') * 64\n",
    "ids, vocab = build_ids_byte_level(txt)\n",
    "ids.shape, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbf763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: overlapping windows for next-token prediction\n",
    "class LMSequenceDataset(Dataset):\n",
    "    def __init__(self, ids: torch.Tensor, block_size: int):\n",
    "        self.ids = ids; self.T = int(block_size)\n",
    "    def __len__(self):\n",
    "        return max(0, self.ids.numel() - self.T)\n",
    "    def __getitem__(self, idx):\n",
    "        i = int(idx)\n",
    "        x = self.ids[i:i+self.T]\n",
    "        y = self.ids[i+1:i+self.T+1]\n",
    "        return x, y\n",
    "ds = LMSequenceDataset(ids, block_size=64)\n",
    "len(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76972249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal positions (reuse from Ch. 8)\n",
    "def sinusoidal_positions(T: int, d_model: int, device=None):\n",
    "    pos = torch.arange(T, device=device).float()[:, None]\n",
    "    i = torch.arange(d_model, device=device).float()[None, :]\n",
    "    ang = pos / (10000 ** (2 * (i//2) / d_model))\n",
    "    enc = torch.zeros(T, d_model, device=device)\n",
    "    enc[:, 0::2] = torch.sin(ang[:, 0::2])\n",
    "    enc[:, 1::2] = torch.cos(ang[:, 1::2])\n",
    "    return enc\n",
    "sinusoidal_positions(4, 8).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93a4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention with mask normalization as in Ch. 8\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.h = num_heads; self.d = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, 3*d_model, bias=False)\n",
    "        self.out = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        B, T, Dm = x.shape\n",
    "        qkv = self.qkv(x); q, k, v = qkv.chunk(3, dim=-1)\n",
    "        def split(t):\n",
    "            return t.view(B, T, self.h, self.d).transpose(1, 2)\n",
    "        q, k, v = map(split, (q, k, v))\n",
    "        sdpa_mask = None\n",
    "        if mask is not None:\n",
    "            if mask.dim() == 2:\n",
    "                base = (mask == 0).bool()[None, None, :, :]\n",
    "                sdpa_mask = base.expand(B, self.h, T, T)\n",
    "            elif mask.dim() == 3:\n",
    "                base = (mask == 0).bool().unsqueeze(1)\n",
    "                sdpa_mask = base.expand(B, self.h, T, T)\n",
    "            elif mask.dim() == 4:\n",
    "                if mask.size(1) == 1:\n",
    "                    sdpa_mask = (mask == 0).bool().expand(B, self.h, T, T)\n",
    "                else:\n",
    "                    sdpa_mask = (mask == 0).bool()\n",
    "        attn = F.scaled_dot_product_attention(q, k, v, attn_mask=sdpa_mask)\n",
    "        attn = self.drop(attn)\n",
    "        y = attn.transpose(1, 2).contiguous().view(B, T, Dm)\n",
    "        return self.out(y)\n",
    "MultiHeadAttention(32, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed-forward, residual, block, config, and GPT (compact)\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model), nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__(); self.norm = nn.LayerNorm(d_model)\n",
    "    def forward(self, x, sub, *a, **k): return x + sub(self.norm(x), *a, **k)\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, d_ff, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, n_head, dropout)\n",
    "        self.ffn = FeedForward(d_model, d_ff, dropout)\n",
    "        self.res1 = Residual(d_model); self.res2 = Residual(d_model)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.res1(x, self.mha, mask); x = self.res2(x, self.ffn); return x\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    vocab_size: int; block_size: int; d_model: int = 128; n_head: int = 4\n",
    "    n_layer: int = 2; d_ff: int = 512; dropout: float = 0.1\n",
    "    pos_type: str = 'learned'; tie_weights: bool = True\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__(); self.cfg = cfg\n",
    "        V, Tm, D = cfg.vocab_size, cfg.block_size, cfg.d_model\n",
    "        self.tok = nn.Embedding(V, D)\n",
    "        self.pos = nn.Embedding(Tm, D) if cfg.pos_type=='learned' else None\n",
    "        self.drop = nn.Dropout(cfg.dropout)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(D, cfg.n_head, cfg.d_ff, cfg.dropout)\n",
    "            for _ in range(cfg.n_layer)])\n",
    "        self.norm = nn.LayerNorm(D); self.head = nn.Linear(D, V, bias=False)\n",
    "        if cfg.tie_weights: self.head.weight = self.tok.weight\n",
    "        self.apply(self._init)\n",
    "    def _init(self, m):\n",
    "        if isinstance(m, (nn.Linear, nn.Embedding)):\n",
    "            nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.zeros_(m.bias)\n",
    "    def _mask(self, ids):\n",
    "        T = ids.size(1); c = torch.tril(torch.ones(T, T, device=ids.device))\n",
    "        return c\n",
    "    def forward(self, input_ids, targets=None):\n",
    "        B, T = input_ids.size(); x = self.tok(input_ids)\n",
    "        if self.cfg.pos_type=='learned':\n",
    "            pos = torch.arange(T, device=input_ids.device)[None, :]\n",
    "            x = x + self.pos(pos)\n",
    "        else:\n",
    "            x = x + sinusoidal_positions(T, self.cfg.d_model, input_ids.device)[None,:,:]\n",
    "        x = self.drop(x); mask = self._mask(input_ids)\n",
    "        for b in self.blocks: x = b(x, mask)\n",
    "        x = self.norm(x); logits = self.head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            lf = logits.reshape(B*T, -1); tf = targets.reshape(B*T)\n",
    "            loss = F.cross_entropy(lf, tf)\n",
    "        return logits, loss\n",
    "cfg = GPTConfig(vocab_size=vocab, block_size=64, d_model=128, n_head=4,\n",
    "                n_layer=2, d_ff=512, dropout=0.1)\n",
    "model = GPT(cfg).to(device); model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac12c776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader and one batch\n",
    "dl = DataLoader(ds, batch_size=64, shuffle=True, drop_last=True)\n",
    "xb, yb = next(iter(dl))\n",
    "xb.shape, yb.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473085df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One forward/backward/update; print loss\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "xb, yb = xb.to(device), yb.to(device)\n",
    "opt.zero_grad(set_to_none=True)\n",
    "_, loss = model(xb, targets=yb)\n",
    "loss.backward(); opt.step(); float(loss.detach().cpu().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80d84ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Short training run with simple linear warmup and loss curve\n",
    "steps = 200; base_lr = 3e-4; warmup = 50; hist = []\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=base_lr)\n",
    "for i, (x, y) in enumerate(dl):\n",
    "    if i >= steps: break\n",
    "    # linear warmup scale in [0,1]\n",
    "    scale = min(1.0, (i+1)/float(warmup))\n",
    "    for g in opt.param_groups: g['lr'] = base_lr * scale\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    _, loss = model(x, targets=y)\n",
    "    loss.backward(); opt.step()\n",
    "    if i % 20 == 0: print(i, g['lr'], float(loss.detach().cpu().item()))\n",
    "    hist.append(float(loss.detach().cpu().item()))\n",
    "plt.figure(figsize=(5.2,3.2)); plt.plot(hist); plt.title('Training loss');\n",
    "plt.xlabel('step'); plt.ylabel('CE'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62ff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sampling preview (greedy/temperature/top-k)\n",
    "@torch.no_grad()\n",
    "def sample(model, input_ids, max_new_tokens=50, temperature=1.0, top_k=None):\n",
    "    model.eval(); x = input_ids.to(device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        x_cond = x[:, -cfg.block_size:]\n",
    "        logits, _ = model(x_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "        if temperature <= 0:\n",
    "            next_id = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        else:\n",
    "            logits = logits / temperature\n",
    "            if top_k is not None and top_k > 0:\n",
    "                v, _ = torch.topk(logits, top_k)\n",
    "                thr = v[:, [-1]]\n",
    "                logits = torch.where(logits < thr, torch.tensor(-1e9, device=device), logits)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1)\n",
    "        x = torch.cat([x, next_id], dim=1)\n",
    "    return x\n",
    "seed = torch.tensor([[ids[0].item()] * 4], dtype=torch.long, device=device)\n",
    "gen = sample(model, seed, max_new_tokens=16, temperature=0.8, top_k=40)\n",
    "gen.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30703914",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Implement gradient accumulation and confirm loss curves match the non-accumulated baseline.\n",
    "- Add early stopping based on validation loss and test that it triggers appropriately.\n",
    "- Integrate a lightweight experiment tracker (Weights & Biases, TensorBoard, or CSV logger) and log key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f76932",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
