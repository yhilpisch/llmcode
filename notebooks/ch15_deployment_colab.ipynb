{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6e35ea",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18acf81",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch — A Step-by-Step Guide Using Python and PyTorch\n",
    "## Chapter 15 — Deployment & Applications\n",
    "**© Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d5b47",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Package the trained model with lightweight serving infrastructure.\n",
    "- Design latency and throughput tests that match production realities.\n",
    "- Plan monitoring hooks to catch model drift and quality regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e472fa",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "We export the model, wire a minimal API, and simulate traffic patterns to validate performance before launch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae0dd1",
   "metadata": {},
   "source": [
    "### Study Tips\n",
    "\n",
    "Document your deployment assumptions (hardware, concurrency, SLAs). These constraints shape every implementation choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bundle structure (config + weights + tokenizer)\n",
    "import json, torch\n",
    "example = {\n",
    "  'config': {'vocab_size': 32, 'block_size': 8, 'd_model': 32,\n",
    "             'n_head': 4, 'n_layer': 2, 'd_ff': 64, 'dropout': 0.0},\n",
    "  'model_state': {},\n",
    "  'tokenizer': {'level': 'char', 'id_to_token': list(' _abcdefghijklmnopqrstuvwxyz'),\n",
    "                'pad_id': 0, 'unk_id': 1}\n",
    "}\n",
    "list(example.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load + sample from a real bundle (requires repo present and a bundle path)\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path('code').resolve()))\n",
    "from ch09_gpt import GPT, GPTConfig  # type: ignore\n",
    "from ch11_sampling import sample      # type: ignore\n",
    "from ch6_tokenize import SimpleTokenizer, Vocab  # type: ignore\n",
    "\n",
    "def load_bundle(path: str):\n",
    "    b = torch.load(path, map_location='cpu')\n",
    "    cfg = GPTConfig(**b['config'])\n",
    "    model = GPT(cfg).eval(); model.load_state_dict(b['model_state'])\n",
    "    meta = b.get('tokenizer'); tok = None\n",
    "    if meta and meta.get('id_to_token'):\n",
    "        id_to_token = list(meta['id_to_token'])\n",
    "        token_to_id = {t:i for i,t in enumerate(id_to_token)}\n",
    "        vocab = Vocab(token_to_id=token_to_id, id_to_token=id_to_token,\n",
    "                     pad=int(meta.get('pad_id',0)), unk=int(meta.get('unk_id',1)))\n",
    "        tok = SimpleTokenizer(vocab=vocab, level=meta.get('level','char'))\n",
    "    return model, tok\n",
    "\n",
    "def sample_bundle(bundle_path: str, prompt: str,\n",
    "                  max_new=80, temperature=0.9, top_p=0.95, top_k=0):\n",
    "    model, tok = load_bundle(bundle_path)\n",
    "    if tok is None:\n",
    "        ids = torch.tensor([[c for c in prompt.encode('utf-8')]], dtype=torch.long)\n",
    "        out = sample(model, ids, max_new_tokens=max_new, temperature=temperature,\n",
    "                     top_k=(top_k or None), top_p=(top_p or None))\n",
    "        return bytes(out[0].tolist()).decode('utf-8', errors='ignore')\n",
    "    ids = torch.tensor([tok.encode(prompt)], dtype=torch.long)\n",
    "    out = sample(model, ids, max_new_tokens=max_new, temperature=temperature,\n",
    "                 top_k=(top_k or None), top_p=(top_p or None))\n",
    "    return tok.decode(out[0].tolist())\n",
    "\n",
    "# Example (requires a real bundle):\n",
    "# sample_bundle('model_bundle.pt', 'Hello')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03b3efd",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Build a FastAPI or Flask microservice that serves one of the trained checkpoints.\n",
    "- Design a canary analysis comparing the deployed model to a previous baseline.\n",
    "- Draft a monitoring plan that includes automated alerts and qualitative spot checks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e1a39f",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
