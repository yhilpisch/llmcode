{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Chapter 15 \u2014 Deployment & Applications (Colab)\n\n", "Export a bundle, validate it, load it, and sample. These cells mirror the\n", "chapter\u2019s Streamlit/FastAPI wiring so you can sanity\u2011check a bundle inside\n", "a notebook before shipping it.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Bundle structure (config + weights + tokenizer)\n", "import json, torch\n", "example = {\n", "  'config': {'vocab_size': 32, 'block_size': 8, 'd_model': 32,\n", "             'n_head': 4, 'n_layer': 2, 'd_ff': 64, 'dropout': 0.0},\n", "  'model_state': {},\n", "  'tokenizer': {'level': 'char', 'id_to_token': list(' _abcdefghijklmnopqrstuvwxyz'),\n", "                'pad_id': 0, 'unk_id': 1}\n", "}\n", "list(example.keys())\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load + sample from a real bundle (requires repo present and a bundle path)\n", "import sys, pathlib\n", "sys.path.append(str(pathlib.Path('code').resolve()))\n", "from ch09_gpt import GPT, GPTConfig  # type: ignore\n", "from ch11_sampling import sample      # type: ignore\n", "from ch6_tokenize import SimpleTokenizer, Vocab  # type: ignore\n", "\n", "def load_bundle(path: str):\n", "    b = torch.load(path, map_location='cpu')\n", "    cfg = GPTConfig(**b['config'])\n", "    model = GPT(cfg).eval(); model.load_state_dict(b['model_state'])\n", "    meta = b.get('tokenizer'); tok = None\n", "    if meta and meta.get('id_to_token'):\n", "        id_to_token = list(meta['id_to_token'])\n", "        token_to_id = {t:i for i,t in enumerate(id_to_token)}\n", "        vocab = Vocab(token_to_id=token_to_id, id_to_token=id_to_token,\n", "                     pad=int(meta.get('pad_id',0)), unk=int(meta.get('unk_id',1)))\n", "        tok = SimpleTokenizer(vocab=vocab, level=meta.get('level','char'))\n", "    return model, tok\n", "\n", "def sample_bundle(bundle_path: str, prompt: str,\n", "                  max_new=80, temperature=0.9, top_p=0.95, top_k=0):\n", "    model, tok = load_bundle(bundle_path)\n", "    if tok is None:\n", "        ids = torch.tensor([[c for c in prompt.encode('utf-8')]], dtype=torch.long)\n", "        out = sample(model, ids, max_new_tokens=max_new, temperature=temperature,\n", "                     top_k=(top_k or None), top_p=(top_p or None))\n", "        return bytes(out[0].tolist()).decode('utf-8', errors='ignore')\n", "    ids = torch.tensor([tok.encode(prompt)], dtype=torch.long)\n", "    out = sample(model, ids, max_new_tokens=max_new, temperature=temperature,\n", "                 top_k=(top_k or None), top_p=(top_p or None))\n", "    return tok.decode(out[0].tolist())\n", "\n", "# Example (requires a real bundle):\n", "# sample_bundle('model_bundle.pt', 'Hello')\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}