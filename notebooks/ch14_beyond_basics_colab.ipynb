{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6b63b7",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94abd998",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch — A Step-by-Step Guide Using Python and PyTorch\n",
    "## Chapter 14 — Beyond the Basics\n",
    "**© Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf3e0b",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Explore advanced training techniques such as curriculum learning or reinforcement learning signals.\n",
    "- Benchmark new architectures against your baseline transformer.\n",
    "- Identify which techniques are worth productizing versus leaving as research spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aff0d3",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "We survey techniques that push performance beyond the vanilla transformer, highlighting when to reach for each lever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed812ae9",
   "metadata": {},
   "source": [
    "### Study Tips\n",
    "\n",
    "Keep track of assumptions. Advanced tricks often rely on dataset or objective specifics—note them explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f628db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: Torch, plotting style, device, and contextlib\n",
    "import torch, contextlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8')\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "torch.manual_seed(0)\n",
    "device = ('cuda' if torch.cuda.is_available() else\n",
    "          'mps' if getattr(torch.backends, 'mps', None) and\n",
    "                   torch.backends.mps.is_available() else 'cpu')\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA: low-rank adapters for a Linear layer\n",
    "class LoRALinear(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_out, r=8, alpha=16.0, bias=False):\n",
    "        \"\"\"Create a Linear with LoRA adapters.\n",
    "        Only the small A (r x d_in) and B (d_out x r) train.\n",
    "        scale = alpha / r.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base = torch.nn.Linear(d_in, d_out, bias=bias)\n",
    "        self.r = int(r); self.scale = float(alpha) / max(1, int(r))\n",
    "        if self.r > 0:\n",
    "            self.A = torch.nn.Linear(d_in, self.r, bias=False)\n",
    "            self.B = torch.nn.Linear(self.r, d_out, bias=False)\n",
    "            torch.nn.init.kaiming_uniform_(self.A.weight, a=2**0.5)\n",
    "            torch.nn.init.zeros_(self.B.weight)\n",
    "            for p in self.base.parameters(): p.requires_grad = False\n",
    "        else:\n",
    "            self.A = None; self.B = None\n",
    "        self.merged = False\n",
    "    def forward(self, x):\n",
    "        y = self.base(x)\n",
    "        if self.r > 0 and not self.merged: y = y + self.scale * self.B(self.A(x))\n",
    "        return y\n",
    "    @torch.no_grad()\n",
    "    def merge(self):\n",
    "        \"\"\"Fold LoRA delta into base weight for inference.\n",
    "        \"\"\"\n",
    "        if self.r == 0 or self.merged: self.merged = True; return\n",
    "        delta = (self.B.weight @ self.A.weight) * self.scale\n",
    "        self.base.weight += delta; self.merged = True\n",
    "LoRALinear(16, 16, r=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908fc722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter counts and trainables\n",
    "def count_params(m): return sum(p.numel() for p in m.parameters())\n",
    "def count_trainable(m): return sum(p.numel() for p in m.parameters()\n",
    "                                 if p.requires_grad)\n",
    "base = torch.nn.Linear(256, 256, bias=False)\n",
    "lora = LoRALinear(256, 256, r=8, alpha=16, bias=False)\n",
    "count_params(base), count_trainable(base),\n",
    "count_params(lora), count_trainable(lora)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c215097a",
   "metadata": {},
   "source": [
    "### LoRA mini‑training demo\n",
    "\n",
    "Train only A/B to fit a small mapping; compare to a fully\n",
    "trainable Linear as a sanity check.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f536e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic mapping: y = target @ x^T (no bias)\n",
    "torch.manual_seed(0)\n",
    "d=64; target = torch.randn(d, d)\n",
    "X = torch.randn(256, d); Y = X @ target.t()\n",
    "# Full linear vs LoRA (frozen base)\n",
    "full = torch.nn.Linear(d, d, bias=False)\n",
    "lora = LoRALinear(d, d, r=8, alpha=16, bias=False)\n",
    "opt_full = torch.optim.Adam(full.parameters(), lr=3e-2)\n",
    "opt_lora = torch.optim.Adam([p for p in lora.parameters() if p.requires_grad], lr=3e-2)\n",
    "losses_f, losses_l = [], []\n",
    "for step in range(60):\n",
    "    # full\n",
    "    opt_full.zero_grad(); yhat = full(X); lf = ((yhat - Y)**2).mean(); lf.backward(); opt_full.step()\n",
    "    # lora\n",
    "    opt_lora.zero_grad(); yhat2 = lora(X); ll = ((yhat2 - Y)**2).mean(); ll.backward(); opt_lora.step()\n",
    "    if step % 10 == 0: losses_f.append(float(lf.detach())); losses_l.append(float(ll.detach()))\n",
    "losses_f, losses_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot LoRA vs full Linear losses collected every 10 steps\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(4.6,2.8));\n",
    "plt.plot([i*10 for i in range(len(losses_f))], losses_f, label='full', color='#0A66C2');\n",
    "plt.plot([i*10 for i in range(len(losses_l))], losses_l, label='lora', color='#DD4444');\n",
    "plt.title('Mini training: full vs LoRA'); plt.xlabel('step'); plt.ylabel('MSE');\n",
    "plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dae61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tiny weight-only quantization demo (per-tensor, symmetric)\n",
    "def fake_int8_weight_only(W):\n",
    "    \"\"\"Return (q:int8, scale:float) with W ≈ scale * q.\n",
    "    \"\"\"\n",
    "    maxv = W.abs().max().clamp(min=1e-8)\n",
    "    scale = float(maxv / 127.0)\n",
    "    q = torch.clamp((W / scale).round(), -127, 127).to(torch.int8)\n",
    "    return q, scale\n",
    "def dequant(q, scale): return q.float() * scale\n",
    "W = torch.randn(256, 256)\n",
    "q, s = fake_int8_weight_only(W); rec = dequant(q, s)\n",
    "float((W - rec).abs().mean())\n",
    "\n",
    "# Error histogram\n",
    "import matplotlib.pyplot as plt\n",
    "err = (W - rec).view(-1).detach().numpy()\n",
    "plt.figure(figsize=(4.2,2.6)); plt.hist(err, bins=40, color='#0A66C2');\n",
    "plt.title('Quantization error'); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6bc2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distillation step with temperature; robust to modules returning\n",
    "# logits or (logits, loss).\n",
    "import torch.nn.functional as F\n",
    "def _forward_logits(m, x):\n",
    "    out = m(x)\n",
    "    return out[0] if isinstance(out, (tuple, list)) else out\n",
    "def distill_step(student, teacher, x, T=2.0, lam=0.5):\n",
    "    \"\"\"Return distillation loss (batchmean KL scaled by T^2).\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): t_logits = _forward_logits(teacher, x)\n",
    "    s_logits = _forward_logits(student, x)\n",
    "    t = F.log_softmax(t_logits / T, dim=-1)\n",
    "    s = F.log_softmax(s_logits / T, dim=-1)\n",
    "    kl = F.kl_div(s, t, log_target=True, reduction='batchmean') * (T*T)\n",
    "    return lam * kl\n",
    "teacher = torch.nn.Linear(32, 32)\n",
    "student = torch.nn.Linear(32, 32)\n",
    "x = torch.randn(8, 32)\n",
    "float(distill_step(student, teacher, x).detach().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e613d854",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Prototype curriculum learning by varying sequence length during training.\n",
    "- Add reinforcement learning from human feedback (RLHF) stubs and document prerequisites.\n",
    "- Summarize the pros/cons of two advanced techniques you trialed, including resource implications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8113a92",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
