{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b78ecf",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfbd736",
   "metadata": {},
   "source": [
    "# Building a Large Language Model from Scratch \u2014 A Step-by-Step Guide Using Python and PyTorch\n",
    "## Appendix \u2014 Tokenization Methods: Concepts, Trade-Offs, and Hands-On Examples\n",
    "**\u00a9 Dr. Yves J. Hilpisch**<br>AI-Powered by GPT-5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b349d8",
   "metadata": {},
   "source": [
    "## How to Use This Notebook\n",
    "\n",
    "- Experiment with multiple tokenization strategies using a shared mini-corpus.\n",
    "- Quantify how vocabulary size influences sequence length and downstream costs.\n",
    "- Export tokenizers in formats that can plug into the training notebooks later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ec3e8",
   "metadata": {},
   "source": [
    "### Roadmap\n",
    "\n",
    "You will start with whitespace and rule-based tokenizers, move to byte-pair approaches, and finish by comparing statistics that drive modeling decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb01dad",
   "metadata": {},
   "source": [
    "### Study Tips\n",
    "\n",
    "Keep a scratchpad of tokenization artifacts (vocabularies, merges, stats). The qualitative differences are easier to grasp when you can inspect them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb598df",
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install optional packages if missing (Colab-friendly)\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import tokenizers  # type: ignore\n",
    "    import sentencepiece  # type: ignore\n",
    "except Exception:\n",
    "    if os.environ.get('COLAB_RELEASE_TAG'):\n",
    "        subprocess.run(\n",
    "            [\n",
    "                sys.executable,\n",
    "                '-m',\n",
    "                'pip',\n",
    "                'install',\n",
    "                '-q',\n",
    "                'tokenizers',\n",
    "                'sentencepiece',\n",
    "            ],\n",
    "            check=True,\n",
    "        )\n",
    "        import tokenizers  # type: ignore  # noqa: F401\n",
    "        import sentencepiece  # type: ignore  # noqa: F401\n",
    "    else:\n",
    "        print(\n",
    "            'tokenizers/sentencepiece missing (skipped install outside Colab). '\n",
    "            'Some cells will be illustrative only.'\n",
    "        )\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from IPython import get_ipython  # type: ignore\n",
    "    ip = get_ipython()\n",
    "    if ip is not None:\n",
    "        ip.run_line_magic('config', \"InlineBackend.figure_format = 'svg'\")\n",
    "except Exception:\n",
    "    pass\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5a2c29",
   "metadata": {},
   "source": [
    "## Character and Byte Level\n",
    "Simple, robust baselines; every codepoint (or byte) is a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162890de",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The model dreams in tokens.'\n",
    "chars = [ord(c) for c in text]\n",
    "recovered = ''.join(chr(i) for i in chars)\n",
    "chars[:10], recovered[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a43421d",
   "metadata": {},
   "source": [
    "## Word / Whitespace Split\n",
    "Compact but language-dependent and OOV-prone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def words(s): return re.findall(r'\\b\\w+\\b', s.lower())\n",
    "vocab = {}\n",
    "def encode_words(s):\n",
    "    ids = []\n",
    "    for w in words(s):\n",
    "        if w not in vocab: vocab[w] = len(vocab)\n",
    "        ids.append(vocab[w])\n",
    "    return ids\n",
    "text = 'The model dreams in tokens. The model learns.'\n",
    "encode_words(text), vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ddd40",
   "metadata": {},
   "source": [
    "## Subword (BPE) with tokenizers\n",
    "Train a tiny BPE tokenizer on a miniature corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5800e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "texts = ['The model dreams in tokens.', 'The model learns.']\n",
    "tok = Tokenizer(BPE(unk_token='<unk>'))\n",
    "tok.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=200, special_tokens=['<unk>', '<pad>'])\n",
    "tok.train_from_iterator(texts, trainer)\n",
    "enc = tok.encode('The model models tokens')\n",
    "enc.tokens, enc.ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff387c77",
   "metadata": {},
   "source": [
    "## SentencePiece (BPE)\n",
    "Train a tiny SentencePiece model and tokenize a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e433b169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a tiny SentencePiece model (skips when package is unavailable)\n",
    "try:\n",
    "    import sentencepiece as spm\n",
    "except Exception as exc:\n",
    "    print('sentencepiece not installed; skipping SentencePiece demo.', exc)\n",
    "else:\n",
    "    with open('spm_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write('The model dreams in tokens.\\nThe model learns.\\n')\n",
    "\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input='spm_corpus.txt',\n",
    "        model_prefix='spm_demo',\n",
    "        model_type='bpe',\n",
    "        vocab_size=40,\n",
    "        pad_id=0,\n",
    "        unk_id=1,\n",
    "        bos_id=-1,\n",
    "        eos_id=-1,\n",
    "    )\n",
    "    sp = spm.SentencePieceProcessor(model_file='spm_demo.model')\n",
    "    ids = sp.encode('The model models tokens', out_type=int)\n",
    "    print(sp.id_to_piece(ids))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4f907d",
   "metadata": {},
   "source": [
    "## Quick Visualization: Token Lengths\n",
    "Plot token counts under different tokenizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0acdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'The model models tokens'\n",
    "char_n = len(list(sent))\n",
    "word_n = len(sent.split())\n",
    "bpe_n = len(tok.encode(sent).ids)\n",
    "plt.bar(['char', 'word', 'bpe'], [char_n, word_n, bpe_n],\n",
    "        color=['#DCE6F8', '#CFE2FF', '#B5D0F5'])\n",
    "plt.ylabel('tokens')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1948f00",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Swap in a domain-specific corpus and measure how token length distributions shift.\n",
    "- Implement a simple normalization preprocessor and quantify its effect on vocabulary size.\n",
    "- Compare BPE and unigram tokenizers on the same dataset; document pros, cons, and when you would choose each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebef857",
   "metadata": {},
   "source": [
    "<img src=\"https://theaiengineer.dev/tae_logo_gw_flatter.png\" width=35% align=right>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "appendix_tokenization_colab"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}