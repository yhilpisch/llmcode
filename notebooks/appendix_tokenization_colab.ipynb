{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Appendix: Tokenization Methods (Colab)\n\nThis notebook mirrors the appendix on tokenization. It is self-contained and runnable on Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {"id": "install"},
      "outputs": [],
      "source": [
        "# Install optional packages if missing (Colab-friendly)\n",
        "try:\n",
        "    import tokenizers, sentencepiece  # type: ignore\n",
        "except Exception:\n",
        "    %pip -q install tokenizers sentencepiece\n",
        "import matplotlib.pyplot as plt\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "plt.style.use('seaborn-v0_8')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Character and Byte Level\n",
        "Simple, robust baselines; every codepoint (or byte) is a token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "text = 'The model dreams in tokens.'\n",
        "chars = [ord(c) for c in text]\n",
        "recovered = ''.join(chr(i) for i in chars)\n",
        "chars[:10], recovered[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Word / Whitespace Split\n",
        "Compact but language-dependent and OOV-prone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "def words(s): return re.findall(r'\\b\\w+\\b', s.lower())\n",
        "vocab = {}\n",
        "def encode_words(s):\n",
        "    ids = []\n",
        "    for w in words(s):\n",
        "        if w not in vocab: vocab[w] = len(vocab)\n",
        "        ids.append(vocab[w])\n",
        "    return ids\n",
        "text = 'The model dreams in tokens. The model learns.'\n",
        "encode_words(text), vocab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subword (BPE) with tokenizers\n",
        "Train a tiny BPE tokenizer on a miniature corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "texts = ['The model dreams in tokens.', 'The model learns.']\n",
        "tok = Tokenizer(BPE(unk_token='<unk>'))\n",
        "tok.pre_tokenizer = Whitespace()\n",
        "trainer = BpeTrainer(vocab_size=200, special_tokens=['<unk>', '<pad>'])\n",
        "tok.train_from_iterator(texts, trainer)\n",
        "enc = tok.encode('The model models tokens')\n",
        "enc.tokens, enc.ids\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SentencePiece (BPE)\n",
        "Train a tiny SentencePiece model and tokenize a sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "with open('spm_corpus.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write('The model dreams in tokens.\\nThe model learns.\\n')\n",
        "spm.SentencePieceTrainer.Train(input='spm_corpus.txt', model_prefix='spm_demo', \\n+model_type='bpe', vocab_size=200, pad_id=0, unk_id=1, bos_id=-1, eos_id=-1)\n",
        "sp = spm.SentencePieceProcessor(model_file='spm_demo.model')\n",
        "ids = sp.encode('The model models tokens', out_type=int)\n",
        "sp.id_to_piece(ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick Visualization: Token Lengths\n",
        "Plot token counts under different tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sent = 'The model models tokens'\n",
        "char_n = len(list(sent))\n",
        "word_n = len(sent.split())\n",
        "bpe_n = len(tok.encode(sent).ids)\n",
        "plt.bar(['char','word','bpe'], [char_n, word_n, bpe_n], color=['#DCE6F8','#CFE2FF','#B5D0F5']); plt.ylabel('tokens'); plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {"name": "appendix_tokenization_colab"},
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
    "language_info": {"name": "python", "version": "3"}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

